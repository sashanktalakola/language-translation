{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "language-translation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NtRqTgaRkIxR"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "5KRKt2Oc0csZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Setup"
      ],
      "metadata": {
        "id": "bjP-YuKi27x3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torch==1.8.0 torchtext==0.9.0\n",
        "exit()"
      ],
      "metadata": {
        "id": "cP6yBrc6yAME",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "167a9712-20b6-4cc8-d023-0f2a64369e8a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.8.0\n",
            "  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 735.5 MB 13 kB/s \n",
            "\u001b[?25hCollecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 51.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (4.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2022.6.15)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.0\n",
            "    Uninstalling torchtext-0.13.0:\n",
            "      Successfully uninstalled torchtext-0.13.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.8.0 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0 torchtext-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "am1u474Xm50X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d5fbdc9-bf9c-4108-ee1d-15d4c765202d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at ./drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"./drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## JSON files"
      ],
      "metadata": {
        "id": "lU0qZtE9D8g6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!cp ./drive/MyDrive/projects/language-translation/data/json/language-translation/es-en/es-en-10p*.json ./data/"
      ],
      "metadata": {
        "id": "SKgKvF27Cp8d"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language Translation"
      ],
      "metadata": {
        "id": "psmmoMoq8GPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.legacy.data import Field, BucketIterator, TabularDataset\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from spacy.tokenizer import Tokenizer\n",
        "import spacy\n",
        "import random\n",
        "\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "tTzKLtL5_Ijk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "v8RYUpD_kDm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### seq2seq LSTM"
      ],
      "metadata": {
        "id": "NtRqTgaRkIxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size=100, hidden_size=100, num_layers=2, dropout_rate=.5):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embedding_size = embedding_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_rate)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, X):\n",
        "    embedding = self.dropout(self.embedding(X))\n",
        "    _, (h, c) = self.rnn(embedding)\n",
        "\n",
        "    return h, c\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout_rate):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding_size = embedding_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_rate)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    output_size = input_size\n",
        "    self.fcc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, X, h, c):\n",
        "    X = X.unsqueeze(0)\n",
        "\n",
        "    embedding = self.dropout(self.embedding(X))\n",
        "    output, (h, c) = self.rnn(embedding, (h, c))\n",
        "\n",
        "    prediction = self.fcc(output).squeeze(0)\n",
        "    return prediction, h, c"
      ],
      "metadata": {
        "id": "6pyM0fcdkMqt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class seq2seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super(seq2seq, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, source, target, teacher_force_ratio=.5):\n",
        "    batch_size, target_len = source.shape[1], target.shape[0]\n",
        "    target_vocab_size = len(english.vocab)\n",
        "\n",
        "    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "    h, c = self.encoder(source)\n",
        "    X = target[0]\n",
        "\n",
        "    for i in range(1, target_len):\n",
        "      output, h, c = self.decoder(X, h, c)\n",
        "      outputs[i] = output\n",
        "      predictions = output.argmax(1) #predictions on batch\n",
        "      if random.random() < teacher_force_ratio: X = target[i]\n",
        "      else: X = predictions\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "23RAjMu_kPkH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single Language\n",
        "Spanish to English"
      ],
      "metadata": {
        "id": "LAVBi8mT8Jnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "hutmDMyAUt_r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59e04bc4-2fee-4818-9c0f-b05e73e4a62a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spanish_tokenizer = Tokenizer(spacy.load(\"es_core_news_sm\").vocab)\n",
        "english_tokenizer = Tokenizer(spacy.load(\"en_core_web_sm\").vocab)\n",
        "\n",
        "def tokenizer_es(data):\n",
        "  return [token.text for token in spanish_tokenizer(data)]\n",
        "\n",
        "def tokenizer_en(data):\n",
        "  return [token.text for token in english_tokenizer(data)]"
      ],
      "metadata": {
        "id": "AZ3YhXP6N6ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english = Field(tokenize=\"spacy\", lower=True, sequential=True, use_vocab=True,\n",
        "                init_token=\"<sos>\", eos_token=\"<eos>\", tokenizer_language=\"en_core_web_sm\")\n",
        "\n",
        "spanish = Field(tokenize=\"spacy\", lower=True, sequential=True, use_vocab=True,\n",
        "                init_token=\"<sos>\", eos_token=\"<eos>\", tokenizer_language=\"es_core_news_sm\")\n",
        "\n",
        "fields = {\"spanish\": (\"spanish\", spanish), \"english\": (\"spanish\", english)}"
      ],
      "metadata": {
        "id": "_JbICx56WutC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, valid_data, test_data = TabularDataset.splits(path=\"data\", train=\"es-en-10p-train.json\",\n",
        "                      validation=\"es-en-10p-validation.json\", test=\"es-en-10p-test.json\",\n",
        "                      format=\"json\", fields=fields)"
      ],
      "metadata": {
        "id": "nT758pbRChHk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spanish.build_vocab(train_data, max_size=10000, min_freq=2) #vectors='glove.6B.100d'\n",
        "english.build_vocab(train_data, max_size=10000, min_freq=2) #vectors='glove.6B.100d'\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data),\n",
        "                                                                      batch_size=32, sort_within_batch=True,\n",
        "                                                                      sort_key=lambda x: len(x.spanish))"
      ],
      "metadata": {
        "id": "bg1XdL49COJs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "INPUT_SIZE_ENCODER = len(spanish.vocab)\n",
        "INPUT_SIZE_DECODER = len(english.vocab)\n",
        "OUTPUT_SIZE = INPUT_SIZE_DECODER\n",
        "EMBEDDING_SIZE_ENCODER = 300\n",
        "EMBEDDING_SIZE_DECODER = 300\n",
        "HIDDEN_SIZE = 512\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT_ENCODER = .5\n",
        "DROPOUT_DECODER = .5"
      ],
      "metadata": {
        "id": "n5fHAcC4RmOL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR = .001\n",
        "BATCH_SIZE = 32\n",
        "PAD_INDEX = english.vocab.stoi[\"<pad>\"]\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_INDEX)\n",
        "optimizer = optim.Adam(seq2seq_model.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "CKubZ822fxbX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(INPUT_SIZE_ENCODER, EMBEDDING_SIZE_ENCODER, HIDDEN_SIZE, NUM_LAYERS,\n",
        "                  DROPOUT_ENCODER).to(device)\n",
        "decoder = Decoder(INPUT_SIZE_DECODER, EMBEDDING_SIZE_DECODER, HIDDEN_SIZE, NUM_LAYERS,\n",
        "                  DROPOUT_DECODER).to(device)\n",
        "\n",
        "seq2seq_model = seq2seq(encoder, decoder).to(device)"
      ],
      "metadata": {
        "id": "UkDF6BSogRIa"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}